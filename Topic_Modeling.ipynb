{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03979ff2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-22 16:41:11.828818: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/lsf_10.1.0/10.1/linux3.10-glibc2.17-x86_64/lib:/home/fibebocai/virtualenvs/fibebocai_text/lib/python3.7/site-packages/pyoracleclient/instantclient\n",
      "2022-06-22 16:41:11.828839: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "## import general libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import glob\n",
    "# import nltk\n",
    "#Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "# from gensim.models import coherencemodel\n",
    "from APIs.Data_Cleaning import datacleaning, datamapping, datapreprocessing\n",
    "\n",
    "import nltk\n",
    "nltk.data.path.append(\"/home/fibebocai/nltk_data/\")\n",
    "import en_core_web_sm\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import warnings\n",
    "\n",
    "def fxn():\n",
    "    warnings.warn(\"deprecated\", DeprecationWarning)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    fxn()\n",
    "    \n",
    "import ray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded9036c",
   "metadata": {},
   "source": [
    "<font size =5 color=\"red\">Data Processing</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a5d9c76",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fibebocai/virtualenvs/fibebocai_text/lib/python3.7/site-packages/pandas/core/indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"The oop based class which is responsible for all the data preprocessing needed. If you want to edit more, please find the class\n",
    "in the APIs folder\"\"\"\n",
    "from APIs.Data_Processing import Data_Processing\n",
    "from nltk.corpus import stopwords\n",
    "ticketname = \"Data/tickets (6).xlsx\"\n",
    "stopwords = stopwords.words(\"english\")\n",
    "\n",
    "\"\"\"if you find some words which is causing noise in the topic creation or which are not contributing much in the topic \n",
    "generation then add the word below\"\"\"\n",
    "\n",
    "stopwords.extend(['from', 'subject', 're', 'edu', 'use', 'infineon', 'camstar','com', 'screenshot','yoda','lot','iptester',\\\n",
    "                        \"mkz\",\"mkzsapa\",\"mkzsapaxx\",\"site\",\"xml\", \"total\",\"camstarportal\",\"camstarweb\",\"team\",\"go\",\"gk\",\"gl\",\"alf\",\\\n",
    "                        \"give\",\"zhouyang\",\"morning\",\"get\",\"getquerytext\",\"ghani\",\"unknown\",\"glue\",\"generate\",\"generation\",\"green\",\\\n",
    "                        \"afternoon\",\"good\",\"evening\",\"night\",\"user\",\"freeze\",\"full\",\"forward\",\"hello\",\"baby\",\"id\", \"cannot\",\"unable\",\"add\",\\\n",
    "                        \"flex\", \"error message\", \"error\",\"able\",\"due\",\"mgt\",\"sba\",\"allow\",\"bin\",\"still\",\"workflow\",\"hence\",\"ok\",\"since\",\"critical\",\\\n",
    "                        \"urgent\", \"priority\", \"request\",\"need\", \"main\", \"create\",\"would\",\"like\",\"track\",\"must\",\\\n",
    "                        \"prior\",\"high\",\"find\",\"type\",\"select\", \"job\",\"fa\",\"type\",\"show\", \"status\",\\\n",
    "                        \"move\", \"contact\", \"affected\",\"area\", \"number\",\"server\",\"bth\",\"info\",\"sin\"])\n",
    "a = Data_Processing(ticketname, stopwords) \n",
    "[df_reviews, data_words] = a.getdata()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd70c6b",
   "metadata": {},
   "source": [
    "<font size=5 color=\"red\">sklearn LDA Model</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9f7151",
   "metadata": {},
   "source": [
    "<font size=3 color=\"blue\"> Library Imports for the model</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1088ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Libraries relevant to the model only\"\"\"\n",
    "import numpy as np\n",
    "from sklearn import decomposition\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import gensim.corpora as corpora\n",
    "import pandas as pd\n",
    "\n",
    "\"\"\"The oop based class which I created for the purpose of training and testing of the LDA model of sklearn\"\"\"\n",
    "from APIs.sklearnLDA import sklearnLDA_NMF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851f0593",
   "metadata": {},
   "source": [
    "<font size=3 color=\"blue\">Hyperparameters</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "445e1dc8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_topics = 20\n",
    "num_words = 15\n",
    "learning_decay = 0.9\n",
    "alpha = 0.001\n",
    "eta = \"default\"\n",
    "learning_method = 'batch'\n",
    "batch_size = 1028\n",
    "learning_offset = 10\n",
    "n_top_words = 12\n",
    "epochs = 200\n",
    "random_state = 50\n",
    "ngram_range =(1,3)\n",
    "max_df = 0.9\n",
    "min_df = \"default\"\n",
    "max_features = 500\n",
    "lda_topics = [str(i) for i in range(num_topics)]\n",
    "\n",
    "\"\"\" for saving the hyperparameters\"\"\"\n",
    "\n",
    "h_list = [num_topics, num_words, learning_decay, alpha, eta, learning_method, batch_size, learning_offset,\\\n",
    "         epochs, random_state, ngram_range, max_df, min_df, max_features]\n",
    "columns = [\"num_topics\", \"num_words\",\"learning_decay\", \"alpha\", \"eta\", \"learning_method\",  \"batch_size\",\"learning_offset\", \\\n",
    "           \"epochs\",  \"random_state\", \"ngram_range\",\"max_df\", \"min_df\", \"max_features\",\"lda\",\"nmf\"]\n",
    "# h_dataframe = pd.DataFrame(np.array(h_list), columns = columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffbdd2f",
   "metadata": {},
   "source": [
    "<font size=3 color=\"blue\">Model</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a723428",
   "metadata": {},
   "source": [
    "<font size=3 color = \"green\">LDA model</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1e8598a",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_count = 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "077c8b12",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(sklearnLDA_NMF pid=3412439)\u001b[0m /home/fibebocai/virtualenvs/fibebocai_text/lib/python3.7/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "\u001b[2m\u001b[36m(sklearnLDA_NMF pid=3412439)\u001b[0m   warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "model = \"lda\"\n",
    "b = sklearnLDA_NMF.remote(model,df_reviews, learning_decay, alpha, eta, epochs, learning_method, learning_offset, batch_size, num_topics, num_words, random_state, ngram_range,\\\n",
    "         min_df, max_df, max_features)\n",
    "[lda, vectorizer_cv, X_cv, topics, df_doc_topic, significant_topic] = ray.get(b.topic_gen.remote())\n",
    "df_topic_table = ray.get(b.table_formatting.remote(df_doc_topic))\n",
    "df_topic_table.to_excel(\"Results/df_topic_table_lda_%d.xlsx\"%title_count)\n",
    "ray.get(b.get_model_topics.remote(lda_topics, n_top_words)).to_excel(\"Topics/topics_lda_%d.xlsx\"%title_count)\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48139314",
   "metadata": {},
   "source": [
    "<font size=3 color = \"green\">NMF model</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d54152d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-22 17:09:27,729\tWARNING worker.py:917 -- File descriptor limit 4000 is too low for production servers and may result in connection errors. At least 8192 is recommended. --- Fix with 'ulimit -n 8192'\n",
      "\u001b[2m\u001b[36m(sklearnLDA_NMF pid=3416470)\u001b[0m /home/fibebocai/virtualenvs/fibebocai_text/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1425: FutureWarning: `alpha` was deprecated in version 1.0 and will be removed in 1.2. Use `alpha_W` and `alpha_H` instead\n",
      "\u001b[2m\u001b[36m(sklearnLDA_NMF pid=3416470)\u001b[0m   FutureWarning,\n",
      "\u001b[2m\u001b[36m(sklearnLDA_NMF pid=3416470)\u001b[0m /home/fibebocai/virtualenvs/fibebocai_text/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "\u001b[2m\u001b[36m(sklearnLDA_NMF pid=3416470)\u001b[0m   ConvergenceWarning,\n",
      "\u001b[2m\u001b[36m(sklearnLDA_NMF pid=3416470)\u001b[0m /home/fibebocai/virtualenvs/fibebocai_text/lib/python3.7/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "\u001b[2m\u001b[36m(sklearnLDA_NMF pid=3416470)\u001b[0m   warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "model = \"nmf\"\n",
    "b = sklearnLDA_NMF.remote(model,df_reviews, learning_decay, alpha, eta, epochs, learning_method, learning_offset, batch_size, num_topics, num_words, random_state, ngram_range,\\\n",
    "         min_df, max_df, max_features)\n",
    "[nmf, vectorizer_cv, X_cv, topics, df_doc_topic, significant_topic] = ray.get(b.topic_gen.remote())\n",
    "df_topic_table = ray.get(b.table_formatting.remote(df_doc_topic))\n",
    "df_topic_table.to_excel(\"Results/df_topic_table_nmf_%d.xlsx\"%title_count)\n",
    "ray.get(b.get_model_topics.remote(lda_topics, n_top_words)).to_excel(\"Topics/topics_nmf_%d.xlsx\"%title_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf83deb",
   "metadata": {},
   "source": [
    "<font size=3 color = \"green\">Saving the Hyperparamters</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1de07c9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "h_list.append(\"topic_lda_%d\"%title_count)\n",
    "h_list.append(\"topic_nmf_%d\"%title_count)\n",
    "f = open('hyperparameters.csv', 'a', newline='')\n",
    "writer= csv.writer(f)\n",
    "# writer.writerow(columns)\n",
    "writer.writerow(h_list)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c934b4",
   "metadata": {},
   "source": [
    "<font size=3 color=\"blue\">Testing With The Subcategories</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b01edab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_excel(\"Data/test_data_jcbe.xlsx\")\n",
    "df_test[\"topics\"] = df_test.desc.map(lambda x:b.get_inference(lda_topics, x,0))\n",
    "df_test.to_excel(\"Data/test_result.xlsx\")\n",
    "count = 0\n",
    "for i in range(df_test.shape[0]):\n",
    "    if df_test['topics'].loc[i]==\"2\" or df_test['topics'].loc[i]=='1':\n",
    "        count+=1\n",
    "count/df_test.shape[0]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8268a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_excel(\"Data/eaf_test.xlsx\")\n",
    "df_test[\"topics\"] = df_test.desc.map(lambda x:b.get_inference(lda_topics, x,0))\n",
    "df_test.to_excel(\"Data/test_result_eaf.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc0da43",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in range(df_test.shape[0]):\n",
    "    if df_test['topics'].loc[i]==\"5\" or df_test['topics'].loc[i]=='10' or df_test['topics'].loc[i]=='3':\n",
    "        count+=1\n",
    "count/df_test.shape[0]*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381904f6",
   "metadata": {},
   "source": [
    "<font size =5 color=\"red\">Cluster Transformer</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "206ff6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ClusterTransformer.ClusterTransformer as ctrans\n",
    "import pandas as pd\n",
    "\n",
    "cr = ctrans.ClusterTransformer()\n",
    "model_name = \"allenai/longformer-base-4096\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b21c06b",
   "metadata": {},
   "source": [
    "<font size=3 color=\"blue\">Hyperparameters</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24669b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80e9f62c90364bfc918b9d5cfbdbc0fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/694 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 2\n",
    "max_seq_length = 64\n",
    "convert_to_numpy = False\n",
    "normalize_embeddings = False\n",
    "neighborhood_min_size = 1\n",
    "cutoff_threshold = 0.83\n",
    "kmeans_max_iter = 100\n",
    "kmeans_random_state = 42\n",
    "kmeans_no_cluster = 6\n",
    "li_sentence = df_reviews['Desc'][:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32421868",
   "metadata": {},
   "source": [
    "<font size=3 color=\"blue\">Model</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a8f5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = cr.model_inference(li_sentence, batch_size, model_name, max_seq_length, normalize_embeddings, convert_to_numpy)\n",
    "output_dict = cr.neighborhood_detection(li_sentence, embeddings, cutoff_threshold, neighborhood_min_size)\n",
    "output_kmeans_dict = cr.kmeans_detection(li_sentence, embeddings, kmeans_no_cluster, kmeans_max_iter, kmeans_random_state)\n",
    "neighborhood_detection_df = cr.convert_to_df(output_dict)\n",
    "kmeans_df = cr.convert_to_df(output_kmeans_dict)\n",
    "print(f'DataFrame from neighborhood detection:\\n {neighborhood_detection_df}')\n",
    "print(f'DataFrame from Kmeans detection:\\n {kmeans_df}')\n",
    "cr.plot_cluster(neighborhood_detection_df)\n",
    "cr.plot_cluster(kmeans_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 C16M32G0",
   "language": "python",
   "name": "p3-c8m16-g0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
